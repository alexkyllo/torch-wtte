{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9527ae3a",
   "metadata": {},
   "source": [
    "# WTTE-RNN in PyTorch\n",
    "\n",
    "Alex Kyllo\n",
    "\n",
    "Based on original Keras version written by Egil Martinsson:\n",
    "https://github.com/ragulpr/wtte-rnn/blob/master/examples/keras/simple_example.ipynb\n",
    "MIT license\n",
    "\n",
    "For details, check out\n",
    "https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/\n",
    "https://github.com/ragulpr/wtte-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93eebf2c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_wtte'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_394431/1447001346.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_wtte\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_wtte'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from torch_wtte import losses\n",
    "\n",
    "np.random.seed(11)\n",
    "torch.manual_seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d838c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(n_timesteps, every_nth, n_repeats, noise_level, n_features, use_censored=True):\n",
    "    def get_equal_spaced(n, every_nth):\n",
    "        # create some simple data of evenly spaced events recurring every_nth step\n",
    "        # Each is on (time,batch)-format\n",
    "        events = np.array([np.array(range(n)) for _ in range(every_nth)])\n",
    "        events = events + np.array(range(every_nth)).reshape(every_nth, 1) + 1\n",
    "\n",
    "        tte_actual = every_nth - 1 - events % every_nth\n",
    "\n",
    "        was_event = (events % every_nth == 0) * 1.0\n",
    "        was_event[:, 0] = 0.0\n",
    "\n",
    "        events = tte_actual == 0\n",
    "\n",
    "        is_censored = (events[:, ::-1].cumsum(1)[:, ::-1] == 0) * 1\n",
    "        tte_censored = is_censored[:, ::-1].cumsum(1)[:, ::-1] * is_censored\n",
    "        tte_censored = tte_censored + (1 - is_censored) * tte_actual\n",
    "\n",
    "        events = np.copy(events.T * 1.0)\n",
    "        tte_actual = np.copy(tte_actual.T * 1.0)\n",
    "        tte_censored = np.copy(tte_censored.T * 1.0)\n",
    "        was_event = np.copy(was_event.T * 1.0)\n",
    "        not_censored = 1 - np.copy(is_censored.T * 1.0)\n",
    "\n",
    "        return tte_censored, not_censored, was_event, events, tte_actual\n",
    "\n",
    "    tte_censored, not_censored, was_event, events, tte_actual = get_equal_spaced(\n",
    "        n=n_timesteps, every_nth=every_nth\n",
    "    )\n",
    "\n",
    "    # From https://keras.io/layers/recurrent/\n",
    "    # input shape rnn recurrent if return_sequences: (nb_samples, timesteps, input_dim)\n",
    "\n",
    "    u_train = not_censored.T.reshape(n_sequences, n_timesteps, 1)\n",
    "    x_train = was_event.T.reshape(n_sequences, n_timesteps, 1)\n",
    "    tte_censored = tte_censored.T.reshape(n_sequences, n_timesteps, 1)\n",
    "    y_train = np.append(tte_censored, u_train, axis=2)  # (n_sequences,n_timesteps,2)\n",
    "\n",
    "    u_test = np.ones(shape=(n_sequences, n_timesteps, 1))\n",
    "    x_test = np.copy(x_train)\n",
    "    tte_actual = tte_actual.T.reshape(n_sequences, n_timesteps, 1)\n",
    "    y_test = np.append(tte_actual, u_test, axis=2)  # (n_sequences,n_timesteps,2)\n",
    "\n",
    "    if not use_censored:\n",
    "        x_train = np.copy(x_test)\n",
    "        y_train = np.copy(y_test)\n",
    "    # Since the above is deterministic perfect fit is feasible.\n",
    "    # More noise->more fun so add noise to the training data:\n",
    "\n",
    "    x_train = np.tile(x_train.T, n_repeats).T\n",
    "    y_train = np.tile(y_train.T, n_repeats).T\n",
    "\n",
    "    # Try with more than one feature TODO\n",
    "    x_train_new = np.zeros([x_train.shape[0], x_train.shape[1], n_features])\n",
    "    x_test_new = np.zeros([x_test.shape[0], x_test.shape[1], n_features])\n",
    "    for f in range(n_features):\n",
    "        x_train_new[:, :, f] = x_train[:, :, 0]\n",
    "        x_test_new[:, :, f] = x_test[:, :, 0]\n",
    "\n",
    "    x_train = x_train_new\n",
    "    x_test = x_test_new\n",
    "\n",
    "    # xtrain is signal XOR noise with probability noise_level\n",
    "    noise = np.random.binomial(1, noise_level, size=x_train.shape)\n",
    "    x_train = x_train + noise - x_train * noise\n",
    "    return y_train, x_train, y_test, x_test, events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8a70b",
   "metadata": {},
   "source": [
    "### Generate some data\n",
    "\n",
    "* The true event-sequence is evenly spaced points (but we start anywhere in the sequence)\n",
    "* The true feature is (binary) if there was an event in last step\n",
    "* In the training data the feature has added noise\n",
    "* Training TTE is censored. Testing TTE is uncensored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 200\n",
    "n_sequences = every_nth = 80\n",
    "n_features = 1\n",
    "n_repeats = 1000\n",
    "noise_level = 0.005\n",
    "use_censored = True\n",
    "\n",
    "y_train, x_train, y_test, x_test, events = get_data(\n",
    "    n_timesteps, every_nth, n_repeats, noise_level, n_features, use_censored\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54288928",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#### Plots\n",
    "print(\"test shape\", x_test.shape, y_test.shape)\n",
    "plt.imshow(x_test[:, :, :].sum(axis=2) > 0, interpolation=\"none\", cmap=\"Accent\", aspect=\"auto\")\n",
    "plt.title(\"x_test (lagged/deterministic event indicator)\")\n",
    "plt.show()\n",
    "plt.imshow(y_test[:, :, 0], interpolation=\"none\", cmap=\"jet\", aspect=\"auto\")\n",
    "plt.title(\"y_test[:,:,0] actual tte\")\n",
    "plt.show()\n",
    "\n",
    "print(\"train shape\", x_train.shape, y_train.shape)\n",
    "plt.imshow(\n",
    "    x_train[:every_nth, :, :].mean(axis=2), interpolation=\"none\", cmap=\"Accent\", aspect=\"auto\"\n",
    ")\n",
    "plt.title(\"x_train[:every_nth,:,0] (lagged/noisy event indicator)\")\n",
    "plt.show()\n",
    "plt.imshow(y_train[:every_nth, :, 0], interpolation=\"none\", cmap=\"jet\", aspect=\"auto\")\n",
    "plt.title(\"y_train[:every_nth,:,0] censored tte\")\n",
    "plt.show()\n",
    "plt.imshow(y_train[:every_nth, :, 1], interpolation=\"none\", cmap=\"Accent\", aspect=\"auto\")\n",
    "plt.title(\"y_train[:every_nth,:,1] u (non-censoring indicator)\")\n",
    "plt.show()\n",
    "\n",
    "## Example TTE:\n",
    "print(\"Example TTEs\")\n",
    "plt.plot(\n",
    "    y_train[every_nth // 4, :, 0],\n",
    "    label=\"censored tte (train)\",\n",
    "    color=\"black\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=2,\n",
    "    drawstyle=\"steps-post\",\n",
    ")\n",
    "plt.plot(\n",
    "    y_test[every_nth // 4, :, 0],\n",
    "    label=\"actual tte (test)\",\n",
    "    color=\"black\",\n",
    "    linestyle=\"solid\",\n",
    "    linewidth=2,\n",
    "    drawstyle=\"steps-post\",\n",
    ")\n",
    "\n",
    "plt.xlim(0, n_timesteps)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"time to event\")\n",
    "plt.title(\"Example TTEs\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc445ab6",
   "metadata": {},
   "source": [
    "# Train a WTTE-RNN\n",
    "### Things to try out:\n",
    "    * have fun with data paramaters:\n",
    "        * every_nth to control event frequency\n",
    "        * noise_level to make it more noisy\n",
    "        * n_timesteps\n",
    "        * n_features to get more noisy input\n",
    "    * Generate more interesting temporal relationships\n",
    "    * Here we use the smallest possible GRU. Try different learning rates, network architectures, initializations.\n",
    "    * Try Implementing multivariate distributions, other distributions, data pipelines etc.\n",
    "    * Invent better output activation layer\n",
    "    * Invent ways to overcome instability with lots of censoring\n",
    "    * ETC and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramaeters for output activation layer initialization.\n",
    "# Start at naive geometric (beta=1) MLE:\n",
    "tte_mean_train = np.nanmean(y_train[:, :, 0])\n",
    "init_alpha = -1.0 / np.log(1.0 - 1.0 / (tte_mean_train + 1.0))\n",
    "mean_u = np.nanmean(y_train[:, :, 1])\n",
    "init_alpha = init_alpha / mean_u\n",
    "print(\"init_alpha: \", init_alpha, \"mean uncensored: \", mean_u)\n",
    "\n",
    "### Uncomment if you have varying length sequences that is nanpadded to the right:\n",
    "# mask_value = -1.3371337 # Use some improbable but not nan-causing telltale value\n",
    "# x_train[:,:,:][np.isnan(x_train)] = mask_value\n",
    "# y_train[:,:,0][np.isnan(y_train[:,:,0])] = tte_mean_train\n",
    "# y_train[:,:,1][np.isnan(y_train[:,:,1])] = 0.5\n",
    "# sample_weights = (x_train[:,:,0]!=mask_value)*1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a03fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WTTERNN(nn.Module):\n",
    "    def __init__(self, discrete):\n",
    "        super().__init__()\n",
    "        self.epoch = 0\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.GRU(input_size=n_features, hidden_size=2, batch_first=True),\n",
    "                nn.Tanh(),\n",
    "                losses.WeibullActivation(init_alpha=init_alpha, max_beta=4.0),\n",
    "            ]\n",
    "        )\n",
    "        self.criterion = losses.WeibullCensoredNLLLoss(discrete=discrete)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.layers[0](x)  # discard GRU hidden state output\n",
    "        x = self.layers[1](x)\n",
    "        x = self.layers[2](x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, optimizer, train_loader, device=\"cpu\"):\n",
    "        num_batches = (\n",
    "            len(train_loader.dataset) + train_loader.batch_size - 1\n",
    "        ) // train_loader.batch_size\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "        train_losses = []\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = self(data).squeeze()\n",
    "            tte = labels[..., 0]\n",
    "            uncensored = labels[..., 1]\n",
    "            alpha = output[..., 0]\n",
    "            beta = output[..., 1]\n",
    "            loss = self.criterion(tte, uncensored, alpha, beta).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            avg_loss = loss / len(data)\n",
    "            print(\n",
    "                f\"Epoch: {self.epoch} [batch {batch_idx+1}/{num_batches}]\\tLoss: {loss.item():.6f} Avg Loss: {avg_loss:.6f}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "        avg_train_loss = np.sum(train_losses) / len(train_loader.dataset)\n",
    "        print()\n",
    "        self.epoch += 1\n",
    "        return avg_train_loss\n",
    "\n",
    "    def score(self, valid_loader, device=\"cpu\"):\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        valid_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in valid_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                output = self(data).squeeze()\n",
    "                tte = labels[..., 0]\n",
    "                uncensored = labels[..., 1]\n",
    "                alpha = output[..., 0]\n",
    "                beta = output[..., 1]\n",
    "                valid_loss = self.criterion(tte, uncensored, alpha, beta).sum().item()\n",
    "                pred = output.data.round()\n",
    "                correct += pred.eq(labels.data.view_as(pred)).sum()\n",
    "        n = len(valid_loader.dataset)\n",
    "        valid_loss /= n\n",
    "        print(f\"Validation: avg loss: {valid_loss:.4f}\")\n",
    "        return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(x_train, y_train, x_test, y_test, epochs, device):\n",
    "    print(f\"Using device {device}\")\n",
    "    x_train = torch.from_numpy(x_train.astype(\"float32\")).to(device)\n",
    "    x_test = torch.from_numpy(x_test.astype(\"float32\")).to(device)\n",
    "    y_train = torch.from_numpy(y_train.astype(\"float32\")).to(device)\n",
    "    y_test = torch.from_numpy(y_test.astype(\"float32\")).to(device)\n",
    "\n",
    "    train_data = TensorDataset(x_train, y_train)\n",
    "    test_data = TensorDataset(x_test, y_test)\n",
    "    batch_size = x_train.shape[0] // 10\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    model = WTTERNN(discrete=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    train_loss_history = []\n",
    "    valid_loss_history = []\n",
    "    for _ in range(epochs):\n",
    "        train_loss = model.fit(optimizer, train_loader, device=device)\n",
    "        train_loss_history.append(train_loss)\n",
    "        valid_loss = model.score(test_loader, device=device)\n",
    "        valid_loss_history.append(valid_loss)\n",
    "    return model, train_loss_history, valid_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e786b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, train_losses, valid_losses = run(x_train, y_train, x_test, y_test, epochs=60, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"training\")\n",
    "plt.plot(valid_losses, label=\"validation\")\n",
    "plt.title(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8413e499",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "Try out training the model with different levels of noise. With more noise confidence gets lower (smaller beta). With less noise beta goes to maximum value and the predicted mode/peak probability is centered around the actual TTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some parametric predictions\n",
    "print(\"TESTING (no noise in features)\")\n",
    "print(\"(each horizontal line is a sequence)\")\n",
    "predicted = model(torch.from_numpy(x_test.astype(\"float32\")).to(device)).detach().cpu().numpy()\n",
    "print(predicted.shape)\n",
    "\n",
    "plt.imshow(predicted[:, :, 0], interpolation=\"none\", cmap=\"jet\", aspect=\"auto\")\n",
    "plt.title(\"predicted[:,:,0] (alpha)\")\n",
    "plt.colorbar(orientation=\"horizontal\")\n",
    "plt.show()\n",
    "plt.imshow(predicted[:, :, 1], interpolation=\"none\", cmap=\"jet\", aspect=\"auto\")\n",
    "plt.title(\"predicted[:,:,1] (beta)\")\n",
    "plt.colorbar(orientation=\"horizontal\")\n",
    "plt.show()\n",
    "\n",
    "print(\"TRAINING (Noisy features)\")\n",
    "predicted = (\n",
    "    model(torch.from_numpy(x_train[:every_nth, :, :].astype(\"float32\")).to(device))\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .numpy()\n",
    ")\n",
    "print(predicted.shape)\n",
    "\n",
    "plt.imshow(predicted[:, :, 0], interpolation=\"none\", cmap=\"jet\", aspect=\"auto\")\n",
    "plt.title(\"predicted[:,:,0] (alpha)\")\n",
    "plt.colorbar(orientation=\"horizontal\")\n",
    "plt.show()\n",
    "plt.imshow(predicted[:, :, 1], interpolation=\"none\", cmap=\"jet\", aspect=\"auto\")\n",
    "plt.title(\"predicted[:,:,1] (beta)\")\n",
    "plt.colorbar(orientation=\"horizontal\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retention-rnn",
   "language": "python",
   "name": "retention-rnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
